{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"muted\")\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources and Tutorials\n",
    "\n",
    "- [Great feature engineering from a Kaggle kernel](https://www.kaggle.com/humananalog/house-prices-advanced-regression-techniques/xgboost-lasso/code)\n",
    "- [XGBoost tuning tutorial here](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in data\n",
    "train = pd.read_csv(\"data/train.csv\", index_col = 0)\n",
    "test = pd.read_csv(\"data/test.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# separate the labels from the predictors in the training set\n",
    "y = train.loc[:,\"SalePrice\"]\n",
    "train = train.drop(\"SalePrice\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inspect the training set\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inspect the test set\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Exploratory Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(y)\n",
    "plt.xlabel(\"Sale Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col, dtype in zip(train.columns, train.dtypes):\n",
    "    if dtype is not np.dtype(\"object\"):\n",
    "        col_data = train.loc[:,col].dropna()\n",
    "        print(\"{}'s skewness: {:.2f}\".format(col, stats.skew(col_data)))\n",
    "        plt.hist(col_data)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# log the response variable since it's skewed\n",
    "y = np.log(y)\n",
    "\n",
    "# split the features between numeric and categorical\n",
    "train_numeric = train.select_dtypes(exclude = [np.dtype(\"object\")])\n",
    "train_categorical = train.select_dtypes(include = [np.dtype(\"object\")])\n",
    "\n",
    "test_numeric = test.select_dtypes(exclude = [np.dtype(\"object\")])\n",
    "test_categorical = test.select_dtypes(include = [np.dtype(\"object\")])\n",
    "\n",
    "# re-encode the dummy variables \n",
    "\n",
    "# first combine the training and test sets so that the dummy variable encoding will be consistent\n",
    "all_categorical = pd.concat((train_categorical,test_categorical))\n",
    "all_categorical = pd.get_dummies(all_categorical)\n",
    "\n",
    "# separate the training and test sets again (categorical)\n",
    "train_categorical = all_categorical.loc[:train.shape[0],]\n",
    "test_categorical = all_categorical.loc[(train.shape[0]+1):,]\n",
    "\n",
    "# filling in missing numeric values with the mean\n",
    "\n",
    "train_numeric = train_numeric.fillna(train_numeric.mean())\n",
    "test_numeric = test_numeric.fillna(test_numeric.mean())\n",
    "\n",
    "# scale all numeric variables by subtracting by the mean and dividing by the standard deviation \n",
    "# do not include the encoded dummy variables\n",
    "\n",
    "# scaler function preserves the means and standard deviations of the training set to be used on the test set \n",
    "scaler = preprocessing.StandardScaler().fit(train_numeric)\n",
    "\n",
    "train_numeric = pd.DataFrame(scaler.transform(train_numeric), columns = train_numeric.columns, index = train_numeric.index)\n",
    "test_numeric = pd.DataFrame(scaler.transform(test_numeric), columns = test_numeric.columns, index = test_numeric.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge the numeric and categorical features \n",
    "\n",
    "train_scaled = pd.merge(left = train_numeric, right = train_categorical, left_index = True, right_index = True)\n",
    "test_scaled = pd.merge(left = test_numeric, right = test_categorical, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # re-encode the dummy variables \n",
    "\n",
    "# # first combine the training and test sets so that the dummy variable encoding will be consistent\n",
    "# all_data = pd.concat((train,test))\n",
    "# all_data = pd.get_dummies(all_data)\n",
    "\n",
    "# train = all_data.loc[:train.shape[0],]\n",
    "# test = all_data.loc[(train.shape[0]+1):,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# log the response variable since it's skewed\n",
    "\n",
    "# y = np.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # filling in missing values with the mean\n",
    "\n",
    "# train = train.fillna(train.mean())\n",
    "# test = test.fillna(test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # scale all variables by subtracting by the mean and dividing by the standard deviation \n",
    "# # this includes the encoded dummy variables\n",
    "\n",
    "# # scaler function preserves the means and standard deviations of the training set to be used on the test set \n",
    "\n",
    "# scaler = preprocessing.StandardScaler().fit(train)\n",
    "\n",
    "# train_scaled = scaler.transform(train)\n",
    "# test_scaled = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize and fit the model\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(X = train_scaled, y = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict and write out submission\n",
    "results = pd.DataFrame(lm.predict(test_scaled), index = test.index, columns = [\"SalePrice\"])\n",
    "results[\"SalePrice\"] = np.exp(results[\"SalePrice\"])\n",
    "results.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check if there are inf predictions\n",
    "results.sort_values(by = \"SalePrice\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize and fit the model\n",
    "lassocv = linear_model.LassoCV()\n",
    "lassocv.fit(X = train_scaled, y = np.ravel(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict and write out submission\n",
    "results = pd.DataFrame(lassocv.predict(test_scaled), index = test.index, columns = [\"SalePrice\"])\n",
    "results[\"SalePrice\"] = np.exp(results[\"SalePrice\"])\n",
    "results.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize and fit the model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "                 colsample_bytree=0.2,\n",
    "                 gamma=0.0,\n",
    "                 learning_rate=0.01,\n",
    "                 max_depth=4,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=7200,                                                                  \n",
    "                 reg_alpha=0.9,\n",
    "                 reg_lambda=0.6,\n",
    "                 subsample=0.2,\n",
    "                 seed=42)\n",
    "xgb_model.fit(X = train_scaled, y = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict and write out submission\n",
    "\n",
    "results = pd.DataFrame(xgb_model.predict(test_scaled), index = test.index, columns = [\"SalePrice\"])\n",
    "results[\"SalePrice\"] = np.exp(results[\"SalePrice\"])\n",
    "results.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building an sklearn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# why do we need to use a pipeline?\n",
    "# when we normalize our features in the training set and then do cross validation, we're actually \"leaking\" information \n",
    "# feature normalization and cross validation should be wrapped together in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in data\n",
    "train = pd.read_csv(\"data/train.csv\", index_col = 0)\n",
    "test = pd.read_csv(\"data/test.csv\", index_col = 0)\n",
    "\n",
    "# separate the labels from the predictors in the training set\n",
    "y = train.loc[:,\"SalePrice\"]\n",
    "train = train.drop(\"SalePrice\", axis = 1)\n",
    "\n",
    "# log the response variable since it's skewed\n",
    "y = np.log(y)\n",
    "\n",
    "# split the features between numeric and categorical\n",
    "train_numeric = train.select_dtypes(exclude = [np.dtype(\"object\")])\n",
    "train_categorical = train.select_dtypes(include = [np.dtype(\"object\")])\n",
    "\n",
    "test_numeric = test.select_dtypes(exclude = [np.dtype(\"object\")])\n",
    "test_categorical = test.select_dtypes(include = [np.dtype(\"object\")])\n",
    "\n",
    "# re-encode the dummy variables \n",
    "\n",
    "# first combine the training and test sets so that the dummy variable encoding will be consistent\n",
    "all_categorical = pd.concat((train_categorical,test_categorical))\n",
    "all_categorical = pd.get_dummies(all_categorical)\n",
    "\n",
    "# separate the training and test sets again (categorical)\n",
    "train_categorical = all_categorical.loc[:train.shape[0],]\n",
    "test_categorical = all_categorical.loc[(train.shape[0]+1):,]\n",
    "\n",
    "# merge the numeric and categorical features \n",
    "\n",
    "train = pd.merge(left = train_numeric, right = train_categorical, left_index = True, right_index = True)\n",
    "test = pd.merge(left = test_numeric, right = test_categorical, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input to pipeline: training set with encoded categorical variables\n",
    "\n",
    "# 1st pipeline step: filling in NaNs\n",
    "# 2nd pipeline step: standard scaler\n",
    "# 3rd pipeline step: lasso cross validation\n",
    "lasso_pipeline = Pipeline([(\"impute mean\", preprocessing.Imputer()),\n",
    "                           (\"preprocessing\", preprocessing.StandardScaler()),\n",
    "                           (\"lasso\", linear_model.LassoCV())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasso_pipeline.fit(X = train, y = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict and write out submission\n",
    "results = pd.DataFrame(lasso_pipeline.predict(test), index = test.index, columns = [\"SalePrice\"])\n",
    "results[\"SalePrice\"] = np.exp(results[\"SalePrice\"])\n",
    "results.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasso_pipeline.get_params()[\"steps\"][2][1].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  XGBoost Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n",
    "\n",
    "$$\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}{(log(y\\_pred_i) - log(y\\_obs_i))^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the error function used for this competition\n",
    "# we should define a function that takes in y_true and y_pre (single data points)\n",
    "# and outputs the gradient and hessian\n",
    "# objective(y_true, y_pred) -> grad, hess\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    # define the gradient (1st derivative)\n",
    "    grad = 0\n",
    "    # define the hessian (2nd derivative)\n",
    "    hess = 0\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor(\n",
    "                 colsample_bytree=0.2,\n",
    "                 gamma=0.0,\n",
    "                 learning_rate=0.01,\n",
    "                 max_depth=4,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=7200,                                                                  \n",
    "                 reg_alpha=0.9,\n",
    "                 reg_lambda=0.6,\n",
    "                 subsample=0.2,\n",
    "                 seed=42,\n",
    "                )\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "            \n",
    "\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_pipeline = Pipeline([(\"impute mean\", preprocessing.Imputer()),\n",
    "                           (\"preprocessing\", preprocessing.StandardScaler()),\n",
    "                           (\"xgb\", xgb_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_pipeline.fit(X = train, y = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict and write out submission\n",
    "results = pd.DataFrame(xgb_pipeline.predict(test), index = test.index, columns = [\"SalePrice\"])\n",
    "results[\"SalePrice\"] = np.exp(results[\"SalePrice\"])\n",
    "results.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(xgb_pipeline.get_params()[\"steps\"][2][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_model.fit(X = train, y = y)\n",
    "# predict and write out submission\n",
    "results = pd.DataFrame(xgb_model.predict(test), index = test.index, columns = [\"SalePrice\"])\n",
    "results[\"SalePrice\"] = np.exp(results[\"SalePrice\"])\n",
    "results.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# read in data\n",
    "dtrain = xgb.DMatrix('demo/data/agaricus.txt.train')\n",
    "dtest = xgb.DMatrix('demo/data/agaricus.txt.test')\n",
    "# specify parameters via map\n",
    "param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' }\n",
    "num_round = 2\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "# make prediction\n",
    "preds = bst.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
